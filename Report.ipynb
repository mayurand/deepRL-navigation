{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Project 1: Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file reports the method adopted to train an agent for solving the task of picking yellow bananas while avoiding the blue bananas in a Unity environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- state space: 37\n",
    "- action space: 4 \n",
    "- reward structure: +1 for picking yellow banana; -1 for picking blue banana; 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "- The inputs are given to an agent which follows an epsilon greedy policy. \n",
    "- As the state space has real numbers, implementing a Q table is not possible. Hence, the Q values i.e. Q function is approximated using a Neural Network (NN). \n",
    "- The structure of the NN is as follows:\n",
    "    - F1 = ReLU (input_state (states = 37) x 64 neurons)\n",
    "    - F2 = ReLU (F1 x 64 neurons)\n",
    "    - logits = ReLU (F2 x output_state (actions = 4))\n",
    "- The NN outputs the estimated rewards from a given state for all actions. An action from these action values is chosen in an epsilon greedy manner. \n",
    "- Two NNs of the same architecture are used: local network (θ_local) and target network (θ_target).\n",
    "- The weigts of local network are updated as follows:\n",
    "<img src=\"images/loss_function.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "    - where the TD error is calculated using the difference of θ_target - θ_local.\n",
    "\n",
    "- The target network is trained based on the local network θ_target = τ*θ_local + (1 - τ)*θ_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- BUFFER_SIZE = 100000    # replay buffer size\n",
    "- BATCH_SIZE = 64         # minibatch size\n",
    "- GAMMA = 0.99            # discount factor\n",
    "- TAU = 1e-3              # for soft update of target parameters\n",
    "- LR = 5e-4               # learning rate \n",
    "- UPDATE_EVERY = 4        # how often to update the network\n",
    "- maximum number of timesteps per episode =1000, \n",
    "- eps_start=1.0           # Starting epsilon value\n",
    "- eps_end=0.01            # Minimum value of epsilon\n",
    "- eps_decay=0.995         # Epsilon decay rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards plot\n",
    "A plot of the average rewards received is seen below:\n",
    "![alt text](images/training_graph.png \"ABC\")\n",
    "It can be seen that the agent receives higher rewards as the experience i.e. number of episodes increases. \n",
    "\n",
    "Number of episodes needed to solve the environment = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future ideas for improving agents performance\n",
    "- Use a different Neural Network for approximating the Q values\n",
    "- Implement a double DQN, a dueling DQN, or prioritized experience replay for faster and improved agent performance.\n",
    "- Implement using Hierarchical reinforcement learning\n",
    "    - Pick up yellow banana\n",
    "    - Avoid blue banana\n",
    "    - Give location of a yellow banana\n",
    "- Implement using policy gradient methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p1_drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
